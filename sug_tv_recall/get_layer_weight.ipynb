{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "uuid": "a57feea5-22a6-4185-a8c7-58223464bbba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep dense 维度：32\n",
      "wide.wide_linear.weight          torch.Size([1, 17])\n",
      "wide.wide_linear.bias          torch.Size([1])\n",
      "deepdense.embed_layers_dic.emb_layer_category.weight          torch.Size([28, 8])\n",
      "deepdense.embed_layers_dic.emb_layer_family_pred_age_level.weight          torch.Size([1024, 12])\n",
      "deepdense.embed_layers_dic.emb_layer_family_pred_gender.weight          torch.Size([4, 8])\n",
      "deepdense.dense_sequential.dense_layer_0.0.weight          torch.Size([32, 30])\n",
      "deepdense.dense_sequential.dense_layer_0.0.bias          torch.Size([32])\n",
      "deeptext.embedding.weight          torch.Size([180000, 32])\n",
      "deeptext.fc.weight          torch.Size([64, 32])\n",
      "deeptext.fc.bias          torch.Size([64])\n",
      "prefix_embedding.weight          torch.Size([105000, 32])\n",
      "deephead.head_layer_0.0.weight          torch.Size([128, 160])\n",
      "deephead.head_layer_0.0.bias          torch.Size([128])\n",
      "deephead.head_out.weight          torch.Size([1, 128])\n",
      "deephead.head_out.bias          torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "from models.DeepDense import DeepDense\n",
    "from models.TextLSTM import TextLSTM\n",
    "from models.DNNAttr import DNNAttr\n",
    "from models.Wide import Wide\n",
    "\n",
    "from models.WideDeep import WideDeep\n",
    "from optim.Initializer import KaimingNormal, XavierNormal\n",
    "from optim.radam import RAdam\n",
    "from pandas import DataFrame\n",
    "from preprocessing.Preprocessor import WidePreprocessor, DeepPreprocessor, DeepTextPreprocessor, MultiDeepTextPreprocessor\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch, os\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import torch\n",
    "\n",
    "\n",
    "wide = Wide(wide_dim=17, output_dim=1)\n",
    "\n",
    "# init deep_dense model\n",
    "deep_column_idx = dict()\n",
    "deep_column_idx['family_pred_gender'] = 0\n",
    "deep_column_idx['family_pred_age_level'] = 1\n",
    "deep_column_idx['category'] = 2\n",
    "deep_column_idx['ott_uv_norm'] = 3\n",
    "deep_column_idx['category_prefer'] = 4\n",
    "emb_col_val_dim_tuple = []\n",
    "emb_col_val_dim_tuple.append(('family_pred_gender', 4, 8))\n",
    "emb_col_val_dim_tuple.append(('family_pred_age_level', 1024, 12))\n",
    "emb_col_val_dim_tuple.append(('category', 28, 8))\n",
    "deepdense = DeepDense(hidden_layers=[32], dropout=[0.2], deep_column_idx=deep_column_idx, embed_input=emb_col_val_dim_tuple, continuous_cols=['ott_uv_norm', 'category_prefer'])\n",
    "\n",
    "# init transformer model\n",
    "transformer = DNNAttr()\n",
    "\n",
    "wide_deep_model = WideDeep(wide=wide, deepdense=deepdense, deeptext=transformer, head_layers=[128])\n",
    "wide_deep_model.load_state_dict(torch.load('log/05-06_16.29/sug_saved_model.pt'))\n",
    "\n",
    "for name, param in wide_deep_model.named_parameters():\n",
    "    print(name, '        ', param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "uuid": "dff56bc8-47cb-40dc-aef4-810529b7fb82"
   },
   "outputs": [],
   "source": [
    "wide_linear_weight=''\n",
    "wide_linear_bias=''\n",
    "emb_layer_category=''\n",
    "emb_layer_family_pred_age_level=''\n",
    "emb_layer_family_pred_gender=''\n",
    "dense_layer_0_w=''\n",
    "dense_layer_0_bias=''\n",
    "dense_layer_1_w=''\n",
    "dense_layer_1_bias=''\n",
    "query_embedding=''\n",
    "query_embedding_fc_w=''\n",
    "query_embedding_fc_bias=''\n",
    "prefix_embedding=''\n",
    "head_layer_0_w=''\n",
    "head_layer_0_bias=''\n",
    "head_layer_1_w=''\n",
    "head_layer_1_bias=''\n",
    "head_out_w=''\n",
    "head_out_bias=''\n",
    "for name, param in wide_deep_model.named_parameters():\n",
    "    \n",
    "    if 'wide.wide_linear.weight' in name:\n",
    "        wide_linear_weight = np.round(param.detach().numpy(), 3)\n",
    "    \n",
    "    if 'wide.wide_linear.bias' in name:\n",
    "        wide_linear_bias = np.round(param.detach().numpy(), 3)\n",
    "    \n",
    "    # deepdense\n",
    "    if 'deepdense.embed_layers_dic.emb_layer_category.weight' in name:\n",
    "        emb_layer_category = np.round(param.detach().numpy(), 3)\n",
    "    \n",
    "    if 'deepdense.embed_layers_dic.emb_layer_family_pred_age_level.weight' in name:\n",
    "        emb_layer_family_pred_age_level = np.round(param.detach().numpy(), 3)\n",
    "        \n",
    "    if 'deepdense.embed_layers_dic.emb_layer_family_pred_gender.weight' in name:\n",
    "        emb_layer_family_pred_gender = np.round(param.detach().numpy(), 3)\n",
    "    \n",
    "    if 'deepdense.dense_sequential.dense_layer_0.0.weight' in name:\n",
    "        dense_layer_0_w = np.round(param.detach().numpy(), 3)\n",
    "    if 'deepdense.dense_sequential.dense_layer_0.0.bias' in name:\n",
    "        dense_layer_0_bias = np.round(param.detach().numpy(), 3)\n",
    "    if 'deepdense.dense_sequential.dense_layer_1.0.weight' in name:\n",
    "        dense_layer_1_w = np.round(param.detach().numpy(), 3)\n",
    "    if 'deepdense.dense_sequential.dense_layer_1.0.bias' in name:\n",
    "        dense_layer_1_bias = np.round(param.detach().numpy(), 3)\n",
    "    \n",
    "    \n",
    "    # text\n",
    "    if 'deeptext.embedding.weight' in name:\n",
    "        query_embedding = np.round(param.detach().numpy(), 3)\n",
    "    if 'deeptext.fc.weight' in name:\n",
    "        query_embedding_fc_w = np.round(param.detach().numpy(), 3)\n",
    "    if 'deeptext.fc.bias' in name:\n",
    "        query_embedding_fc_bias = np.round(param.detach().numpy(), 3)\n",
    "    \n",
    "    # prefix\n",
    "    if 'prefix_embedding.weight' in name:\n",
    "        prefix_embedding = np.round(param.detach().numpy(), 3)\n",
    "    if 'deephead.head_layer_0.0.weight' in name:\n",
    "        head_layer_0_w = np.round(param.detach().numpy(), 3)\n",
    "    if 'deephead.head_layer_0.0.bias' in name:\n",
    "        head_layer_0_bias = np.round(param.detach().numpy(), 3)\n",
    "    \n",
    "    if 'deephead.head_layer_1.0.weight' in name:\n",
    "        head_layer_1_w = np.round(param.detach().numpy(), 3)\n",
    "    if 'deephead.head_layer_1.0.bias' in name:\n",
    "        head_layer_1_bias = np.round(param.detach().numpy(), 3)\n",
    "    \n",
    "    if 'deephead.head_out.weight' in name:\n",
    "        head_out_w = np.round(param.detach().numpy(), 3)\n",
    "    \n",
    "    if 'deephead.head_out.bias' in name:\n",
    "        head_out_bias = np.round(param.detach().numpy(), 3)\n",
    "    \n",
    "np.savez(\"param.npz\", wide_linear_weight=wide_linear_weight,\n",
    "                        wide_linear_bias=wide_linear_bias,\n",
    "                        emb_layer_category=emb_layer_category,\n",
    "                        emb_layer_family_pred_age_level=emb_layer_family_pred_age_level,\n",
    "                        emb_layer_family_pred_gender=emb_layer_family_pred_gender,\n",
    "                        dense_layer_0_w=dense_layer_0_w,\n",
    "                        dense_layer_0_bias=dense_layer_0_bias,\n",
    "                        dense_layer_1_w=dense_layer_1_w,\n",
    "                        dense_layer_1_bias=dense_layer_1_bias,\n",
    "                        query_embedding=query_embedding,\n",
    "                        query_embedding_fc_w=query_embedding_fc_w,\n",
    "                        query_embedding_fc_bias=query_embedding_fc_bias,\n",
    "                        prefix_embedding=prefix_embedding,\n",
    "                        head_layer_0_w=head_layer_0_w,\n",
    "                        head_layer_0_bias=head_layer_0_bias,\n",
    "                        head_layer_1_w=head_layer_1_w,\n",
    "                        head_layer_1_bias=head_layer_1_bias,\n",
    "                        head_out_w=head_out_w,\n",
    "                        head_out_bias=head_out_bias)\n",
    "    \n",
    "    \n",
    "#     if 'emb_layer_family_pred_age_level' in name:\n",
    "#         emb_layer_family_pred_age_level = np.round(param.detach().numpy(), 3)\n",
    "        \n",
    "#         print(name)\n",
    "#         print(emb_layer_family_pred_age_level)\n",
    "\n",
    "#     if 'wide.wide_linear.weight' in name:\n",
    "#         print(wide_linear_weight)\n",
    "        \n",
    "        \n",
    "np_file = np.load(\"param.npz\")\n",
    "print(np_file['query_embedding'].shape)\n",
    "print(np_file['query_embedding'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "uuid": "3bd05ad3-002a-461f-9881-fc519e080ed0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.037 -0.07   0.191 -0.085 -0.075 -0.149  0.01   0.123  0.152  0.067\n",
      "  -0.24  -0.146  0.18  -0.264 -0.054  0.262 -0.257 -0.032 -0.061 -0.056\n",
      "   0.178  0.311 -0.277 -0.124 -0.401  0.112  0.153  0.029 -0.213 -0.218]\n",
      " [ 0.003  0.2    0.071  0.149  0.133 -0.142 -0.322  0.204 -0.238  0.297\n",
      "  -0.024  0.067 -0.03  -0.155  0.163  0.094 -0.136 -0.326  0.048  0.09\n",
      "   0.164  0.012 -0.128  0.277  0.121  0.152  0.004  0.126  0.154  0.144]\n",
      " [ 0.298  0.151 -0.082 -0.084  0.054 -0.289  0.225  0.129  0.004  0.089\n",
      "  -0.194  0.217  0.04   0.198  0.078 -0.079  0.026 -0.135 -0.136 -0.073\n",
      "  -0.131 -0.16  -0.08   0.066 -0.23   0.259 -0.163 -0.195  0.029 -0.313]\n",
      " [ 0.169 -0.163  0.077 -0.052  0.113  0.031  0.181 -0.216  0.277  0.15\n",
      "  -0.214 -0.268 -0.024 -0.174 -0.208 -0.14   0.121  0.051 -0.058  0.153\n",
      "  -0.219  0.244 -0.049 -0.342 -0.184  0.214  0.025  0.126 -0.047  0.074]\n",
      " [-0.049 -0.008  0.196  0.35   0.047 -0.439 -0.171  0.007 -0.081 -0.094\n",
      "   0.076 -0.084 -0.071  0.004 -0.043  0.092 -0.002 -0.19  -0.065  0.056\n",
      "   0.104  0.216  0.105 -0.082 -0.155  0.099 -0.046  0.053 -0.052 -0.191]\n",
      " [-0.     0.215 -0.299 -0.316 -0.148  0.089  0.023  0.081 -0.072 -0.03\n",
      "  -0.424  0.017 -0.098  0.009 -0.257 -0.015  0.325 -0.309  0.054  0.341\n",
      "  -0.09   0.187 -0.019 -0.382 -0.203 -0.02  -0.303 -0.173 -0.07  -0.205]\n",
      " [ 0.075  0.213  0.14  -0.009 -0.15  -0.103  0.156 -0.081  0.378  0.188\n",
      "  -0.293 -0.314  0.096  0.091 -0.028  0.071 -0.086  0.274  0.277 -0.057\n",
      "  -0.137 -0.579  0.001  0.232  0.024 -0.115  0.054 -0.065 -0.184  0.181]\n",
      " [-0.115  0.165 -0.036 -0.056  0.003 -0.318 -0.275 -0.041  0.186 -0.055\n",
      "  -0.259  0.125  0.036  0.034  0.055 -0.126 -0.019  0.221 -0.438 -0.009\n",
      "  -0.022  0.039 -0.05  -0.248  0.169 -0.374 -0.097 -0.06  -0.     0.052]\n",
      " [ 0.021  0.175  0.065 -0.241 -0.105 -0.162  0.205 -0.006 -0.099 -0.191\n",
      "   0.042  0.192  0.294 -0.096 -0.311  0.01  -0.039 -0.242  0.25  -0.056\n",
      "   0.088  0.311 -0.096  0.154 -0.397 -0.229 -0.052  0.442 -0.041  0.193]\n",
      " [ 0.015 -0.125  0.116 -0.112 -0.195 -0.034 -0.017  0.242  0.099  0.145\n",
      "   0.071  0.025  0.144 -0.313  0.06   0.074 -0.033  0.039  0.192 -0.172\n",
      "   0.112 -0.059 -0.089 -0.036 -0.234 -0.109  0.025 -0.13  -0.    -0.168]\n",
      " [ 0.123 -0.043 -0.177  0.136  0.086  0.028 -0.019  0.096 -0.17   0.141\n",
      "   0.117  0.047  0.116 -0.008  0.095  0.054 -0.217 -0.229 -0.085 -0.077\n",
      "  -0.13  -0.076  0.135 -0.057 -0.069  0.233  0.081  0.287  0.031  0.09 ]\n",
      " [ 0.089 -0.231  0.109 -0.048  0.259 -0.073  0.011  0.109 -0.068 -0.216\n",
      "   0.202 -0.072 -0.28  -0.055  0.043 -0.014 -0.268 -0.067  0.089  0.195\n",
      "  -0.053  0.448  0.16  -0.079  0.064  0.209 -0.226 -0.016  0.308  0.262]\n",
      " [ 0.175  0.188  0.007 -0.045  0.173 -0.208 -0.08   0.109  0.004 -0.01\n",
      "  -0.032  0.16  -0.295  0.03  -0.236 -0.235  0.107 -0.03   0.078 -0.18\n",
      "   0.048  0.323 -0.196 -0.181  0.288 -0.155 -0.084  0.122 -0.243 -0.021]\n",
      " [-0.266  0.029  0.067  0.037  0.321  0.306  0.056 -0.279 -0.165  0.015\n",
      "   0.254  0.065 -0.009  0.017  0.186 -0.197 -0.017 -0.048 -0.029 -0.34\n",
      "  -0.124 -0.153  0.043  0.212 -0.206  0.157  0.144  0.062 -0.026  0.148]\n",
      " [ 0.054  0.132 -0.247  0.229  0.427  0.062  0.017 -0.059 -0.206  0.334\n",
      "  -0.07  -0.062  0.065  0.053  0.048  0.1   -0.023  0.06   0.138 -0.066\n",
      "  -0.141  0.006 -0.39  -0.033 -0.021  0.232  0.23   0.046  0.239 -0.053]\n",
      " [-0.19   0.05  -0.092 -0.337 -0.025 -0.073  0.127 -0.223 -0.047  0.136\n",
      "   0.069  0.02  -0.047 -0.23   0.04   0.088 -0.025 -0.019 -0.208 -0.183\n",
      "   0.13  -0.007 -0.004 -0.05  -0.255 -0.015 -0.008  0.023  0.08  -0.12 ]\n",
      " [ 0.036  0.169 -0.011 -0.423  0.133 -0.345 -0.105 -0.05   0.119  0.179\n",
      "  -0.04   0.302  0.076 -0.275  0.338 -0.046 -0.007  0.3    0.17   0.119\n",
      "  -0.092  0.187 -0.025 -0.176  0.044  0.121 -0.077  0.087  0.001  0.087]\n",
      " [-0.089 -0.24   0.027 -0.295 -0.08   0.205  0.148  0.066  0.072 -0.093\n",
      "  -0.058  0.048  0.391  0.223 -0.18   0.023  0.159 -0.248 -0.083 -0.175\n",
      "   0.007 -0.065  0.076 -0.051  0.115 -0.204 -0.191  0.028  0.1    0.142]\n",
      " [ 0.055 -0.133 -0.104  0.045 -0.092 -0.206  0.113  0.016 -0.121 -0.259\n",
      "  -0.221 -0.171  0.143 -0.17  -0.016 -0.071 -0.152  0.327  0.086 -0.14\n",
      "  -0.143 -0.18   0.256  0.405  0.245  0.089 -0.096  0.077  0.091  0.046]\n",
      " [ 0.103 -0.056 -0.093  0.146 -0.072  0.082 -0.007  0.03   0.095 -0.138\n",
      "  -0.085 -0.143  0.083  0.121  0.013  0.351  0.054 -0.053  0.101  0.235\n",
      "  -0.184 -0.454  0.037 -0.204  0.274 -0.116  0.07  -0.071  0.023  0.114]\n",
      " [-0.234 -0.394  0.402  0.09  -0.098 -0.08  -0.122 -0.088  0.119  0.106\n",
      "   0.038 -0.002  0.073  0.02  -0.254 -0.289  0.162 -0.199  0.166  0.002\n",
      "  -0.003 -0.169  0.121  0.009  0.038 -0.054  0.255  0.308 -0.015  0.119]\n",
      " [ 0.026  0.212 -0.006 -0.058  0.235 -0.093 -0.016  0.027  0.005  0.26\n",
      "  -0.272 -0.2   -0.273  0.124  0.273 -0.09   0.099  0.016  0.002  0.205\n",
      "  -0.047 -0.144  0.125 -0.256  0.191 -0.258  0.024  0.388  0.067 -0.044]\n",
      " [ 0.145  0.032  0.175  0.214 -0.049 -0.152  0.078 -0.11  -0.127  0.103\n",
      "   0.046  0.294 -0.042  0.114 -0.011 -0.315 -0.015 -0.02   0.05  -0.28\n",
      "  -0.074 -0.105  0.098 -0.352  0.016 -0.13  -0.118 -0.211 -0.014  0.105]\n",
      " [ 0.117  0.647  0.029  0.172 -0.287  0.039  0.125 -0.138 -0.275  0.219\n",
      "  -0.092  0.171 -0.239  0.178 -0.081  0.057  0.11   0.133 -0.07   0.036\n",
      "  -0.092 -0.25  -0.082  0.074  0.036 -0.251 -0.24   0.199  0.008 -0.069]\n",
      " [-0.178 -0.307 -0.001 -0.088  0.107  0.039  0.018 -0.209  0.148 -0.098\n",
      "   0.133 -0.114 -0.207 -0.168 -0.036  0.136 -0.207 -0.262 -0.188  0.126\n",
      "  -0.353  0.173 -0.161  0.171 -0.111  0.103 -0.306  0.176 -0.169  0.138]\n",
      " [-0.055  0.156  0.227  0.087 -0.029  0.367  0.049 -0.146  0.188  0.161\n",
      "  -0.077  0.172 -0.184 -0.038  0.069  0.05  -0.015  0.057  0.221  0.04\n",
      "  -0.012  0.047 -0.345 -0.312  0.048 -0.061 -0.009  0.016  0.001 -0.167]\n",
      " [ 0.03  -0.08  -0.047  0.046 -0.067  0.003  0.05   0.078  0.013 -0.094\n",
      "   0.139 -0.015 -0.236 -0.011 -0.062  0.201 -0.208 -0.159 -0.18  -0.068\n",
      "  -0.105 -0.181 -0.167  0.048  0.302  0.302  0.265 -0.066 -0.218  0.024]\n",
      " [-0.011  0.101 -0.315 -0.121  0.192  0.13  -0.289  0.307 -0.141  0.158\n",
      "  -0.074  0.04   0.018 -0.356  0.253 -0.313  0.049 -0.047 -0.101  0.199\n",
      "  -0.072 -0.125  0.269 -0.013  0.27   0.229 -0.187  0.215 -0.1    0.165]\n",
      " [-0.036 -0.088 -0.107 -0.158 -0.054  0.005  0.124 -0.155 -0.12  -0.016\n",
      "   0.051 -0.045 -0.015  0.16  -0.274 -0.014  0.096 -0.035  0.034  0.027\n",
      "  -0.306 -0.425 -0.169  0.317  0.155 -0.143  0.016  0.044 -0.002  0.227]\n",
      " [-0.021 -0.134  0.259  0.004  0.069  0.222 -0.031  0.256  0.343 -0.119\n",
      "  -0.065  0.1    0.042 -0.239 -0.215 -0.067  0.085 -0.13   0.288  0.094\n",
      "  -0.241  0.134 -0.002  0.262  0.226 -0.037 -0.311 -0.025  0.049  0.019]\n",
      " [ 0.394  0.006  0.086  0.156  0.259 -0.401  0.255  0.183  0.17  -0.094\n",
      "   0.25   0.166 -0.24  -0.016  0.359 -0.063  0.176 -0.094  0.016  0.213\n",
      "   0.346  0.118  0.127 -0.011 -0.29  -0.498  0.162 -0.118 -0.211 -0.087]\n",
      " [-0.038  0.01  -0.094 -0.288  0.031 -0.096  0.207 -0.089  0.171 -0.091\n",
      "   0.076  0.053 -0.003  0.119 -0.071 -0.187  0.267  0.249  0.039  0.031\n",
      "   0.467  0.174  0.21  -0.083 -0.043  0.018 -0.058  0.214  0.095 -0.214]]\n"
     ]
    }
   ],
   "source": [
    "# [rows, cols] = np_file['dense_layer_1_bias'].shape\n",
    "\n",
    "print(np_file['dense_layer_0_w'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "uuid": "0c61aee4-333c-4604-ab42-c06cd8589e31"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
